{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91498,"databundleVersionId":11655853,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":11217117,"sourceType":"datasetVersion","datasetId":6988459},{"sourceId":7884485,"sourceType":"datasetVersion","datasetId":4628051},{"sourceId":17191,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":14317,"modelId":21716},{"sourceId":17555,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":14611,"modelId":22086}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n!mkdir -p /root/.cache/torch/hub/checkpoints\n!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install umap-learn\n!pip install hdbscan","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport kornia as K\nimport kornia.feature as KF\nimport h5py\nimport dataclasses\nfrom IPython.display import clear_output\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom lightglue import match_pair\nfrom lightglue import ALIKED, LightGlue\nfrom lightglue.utils import load_image, rbd\nfrom transformers import AutoImageProcessor, AutoModel\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\nimport os\nimport cv2 as cv\nfrom tqdm import tqdm\nfrom time import time, sleep\nimport umap\nfrom hdbscan import HDBSCAN\nimport gc\nimport pandas as pd\n\nimport pycolmap\nimport sys\nsys.path.append('/kaggle/input/imc25-utils')\nfrom database import *\nfrom h5_to_db import *\nimport metric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T22:44:31.815784Z","iopub.execute_input":"2025-04-22T22:44:31.816063Z","iopub.status.idle":"2025-04-22T22:44:44.060663Z","shell.execute_reply.started":"2025-04-22T22:44:31.816041Z","shell.execute_reply":"2025-04-22T22:44:44.059963Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"device = K.utils.get_cuda_device_if_available(0)\nprint(f'{device=}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T22:44:44.061891Z","iopub.execute_input":"2025-04-22T22:44:44.062558Z","iopub.status.idle":"2025-04-22T22:44:44.113134Z","shell.execute_reply.started":"2025-04-22T22:44:44.062524Z","shell.execute_reply":"2025-04-22T22:44:44.112198Z"}},"outputs":[{"name":"stdout","text":"device=device(type='cuda', index=0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"data_dir = \"/kaggle/input/image-matching-challenge-2025\"\ntrain_labels = pd.read_csv(f\"{data_dir}/train_labels.csv\")\ntrain_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T22:44:44.114488Z","iopub.execute_input":"2025-04-22T22:44:44.114730Z","iopub.status.idle":"2025-04-22T22:44:44.144453Z","shell.execute_reply.started":"2025-04-22T22:44:44.114709Z","shell.execute_reply":"2025-04-22T22:44:44.143807Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"             dataset           scene                             image  \\\n0     imc2023_haiper        fountain            fountain_image_116.png   \n1     imc2023_haiper        fountain            fountain_image_108.png   \n2     imc2023_haiper        fountain            fountain_image_101.png   \n3     imc2023_haiper        fountain            fountain_image_082.png   \n4     imc2023_haiper        fountain            fountain_image_071.png   \n...              ...             ...                               ...   \n1940          stairs  stairs_split_2  stairs_split_2_1710453733751.png   \n1941          stairs  stairs_split_2  stairs_split_2_1710453759963.png   \n1942          stairs  stairs_split_2  stairs_split_2_1710453805788.png   \n1943          stairs  stairs_split_2  stairs_split_2_1710453765165.png   \n1944          stairs  stairs_split_2  stairs_split_2_1710453774370.png   \n\n                                        rotation_matrix  \\\n0     0.122655949;0.947713775;-0.294608417;0.1226706...   \n1     0.474305910;0.359108654;-0.803787832;0.2888416...   \n2     0.565115476;-0.138485064;-0.813305838;0.506678...   \n3     -0.308320392;-0.794654112;0.522937261;0.948141...   \n4     -0.569002830;-0.103808175;0.815757098;0.778745...   \n...                                                 ...   \n1940  0.961762441;-0.187990401;0.199179859;-0.177691...   \n1941  0.237960308;0.580896704;-0.778417569;0.4077886...   \n1942  0.309067298;0.541767194;-0.781642957;0.4038963...   \n1943  0.301920210;0.609614467;-0.732949103;0.5007116...   \n1944  0.744374958;0.439048204;-0.503132782;0.4197719...   \n\n                          translation_vector  \n0       0.093771314;-0.803560988;2.062001533  \n1       0.358946647;-0.797557548;1.910906929  \n2       0.146922468;-0.981392596;2.009002852  \n3       0.206413831;-1.174321103;3.667167680  \n4      -0.015140892;-1.334052012;3.488936597  \n...                                      ...  \n1940  -0.112850000;-3.521750000;-2.859750000  \n1941   -0.490768000;-3.064140000;3.008420000  \n1942    -0.572757000;0.885835000;4.987270000  \n1943   -0.135613000;-1.832910000;1.598790000  \n1944   -0.367979000;-0.607935000;0.621483000  \n\n[1945 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>scene</th>\n      <th>image</th>\n      <th>rotation_matrix</th>\n      <th>translation_vector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>imc2023_haiper</td>\n      <td>fountain</td>\n      <td>fountain_image_116.png</td>\n      <td>0.122655949;0.947713775;-0.294608417;0.1226706...</td>\n      <td>0.093771314;-0.803560988;2.062001533</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>imc2023_haiper</td>\n      <td>fountain</td>\n      <td>fountain_image_108.png</td>\n      <td>0.474305910;0.359108654;-0.803787832;0.2888416...</td>\n      <td>0.358946647;-0.797557548;1.910906929</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>imc2023_haiper</td>\n      <td>fountain</td>\n      <td>fountain_image_101.png</td>\n      <td>0.565115476;-0.138485064;-0.813305838;0.506678...</td>\n      <td>0.146922468;-0.981392596;2.009002852</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>imc2023_haiper</td>\n      <td>fountain</td>\n      <td>fountain_image_082.png</td>\n      <td>-0.308320392;-0.794654112;0.522937261;0.948141...</td>\n      <td>0.206413831;-1.174321103;3.667167680</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>imc2023_haiper</td>\n      <td>fountain</td>\n      <td>fountain_image_071.png</td>\n      <td>-0.569002830;-0.103808175;0.815757098;0.778745...</td>\n      <td>-0.015140892;-1.334052012;3.488936597</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1940</th>\n      <td>stairs</td>\n      <td>stairs_split_2</td>\n      <td>stairs_split_2_1710453733751.png</td>\n      <td>0.961762441;-0.187990401;0.199179859;-0.177691...</td>\n      <td>-0.112850000;-3.521750000;-2.859750000</td>\n    </tr>\n    <tr>\n      <th>1941</th>\n      <td>stairs</td>\n      <td>stairs_split_2</td>\n      <td>stairs_split_2_1710453759963.png</td>\n      <td>0.237960308;0.580896704;-0.778417569;0.4077886...</td>\n      <td>-0.490768000;-3.064140000;3.008420000</td>\n    </tr>\n    <tr>\n      <th>1942</th>\n      <td>stairs</td>\n      <td>stairs_split_2</td>\n      <td>stairs_split_2_1710453805788.png</td>\n      <td>0.309067298;0.541767194;-0.781642957;0.4038963...</td>\n      <td>-0.572757000;0.885835000;4.987270000</td>\n    </tr>\n    <tr>\n      <th>1943</th>\n      <td>stairs</td>\n      <td>stairs_split_2</td>\n      <td>stairs_split_2_1710453765165.png</td>\n      <td>0.301920210;0.609614467;-0.732949103;0.5007116...</td>\n      <td>-0.135613000;-1.832910000;1.598790000</td>\n    </tr>\n    <tr>\n      <th>1944</th>\n      <td>stairs</td>\n      <td>stairs_split_2</td>\n      <td>stairs_split_2_1710453774370.png</td>\n      <td>0.744374958;0.439048204;-0.503132782;0.4197719...</td>\n      <td>-0.367979000;-0.607935000;0.621483000</td>\n    </tr>\n  </tbody>\n</table>\n<p>1945 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def pick_images(image_dir, datasets=[]):\n    image_paths = []\n    if len(datasets) == 0:\n        df = train_labels\n    else:\n        df = train_labels[train_labels[\"dataset\"].isin(datasets)]\n    for _, row in df.iterrows():\n        path = os.path.join(image_dir, row[\"dataset\"],row[\"image\"])\n        image_paths.append(path)\n    return image_paths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T22:44:44.145322Z","iopub.execute_input":"2025-04-22T22:44:44.145555Z","iopub.status.idle":"2025-04-22T22:44:44.150001Z","shell.execute_reply.started":"2025-04-22T22:44:44.145536Z","shell.execute_reply":"2025-04-22T22:44:44.149197Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def get_clip_embeddings(image_paths, device):\n    # Load Pretrained CLIP\n    model_name = \"openai/clip-vit-base-patch32\"\n    clip_model = CLIPModel.from_pretrained(model_name).to(device)\n    processor = CLIPProcessor.from_pretrained(model_name)\n\n    def get_image_embeddings(image_path: str):\n        image = Image.open(image_path)\n    \n        with torch.no_grad():\n            inputs = processor(images=image, return_tensors=\"pt\", padding=True).to(device)\n            return clip_model.get_image_features(**inputs)\n\n    # Get image embeddings using CLIP\n    image_embeddings = torch.cat([get_image_embeddings(image_path) for image_path in image_paths])\n\n    # Dimensionality reduction\n    umap_model = umap.UMAP(n_components = 5)\n    reduced_embeddings = umap_model.fit_transform(image_embeddings.detach().cpu().numpy())\n\n    # Cleanup\n    del clip_model\n    del image_embeddings\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return reduced_embeddings\n\ndef cluster_images(reduced_embeddings):\n    # Clustering\n    clusterer = HDBSCAN(min_cluster_size=3)\n    clusterer.fit(reduced_embeddings)\n    clusters = clusterer.labels_\n    return clusters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T22:44:44.150980Z","iopub.execute_input":"2025-04-22T22:44:44.151286Z","iopub.status.idle":"2025-04-22T22:44:44.161015Z","shell.execute_reply.started":"2025-04-22T22:44:44.151265Z","shell.execute_reply":"2025-04-22T22:44:44.160186Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_torch_image(fname, device=torch.device('cpu')):\n    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n    return img\n    \ndef get_img_pairs_exhaustive(img_fnames):\n    index_pairs = []\n    for i in range(len(img_fnames)):\n        for j in range(i+1, len(img_fnames)):\n            index_pairs.append((i,j))\n    return index_pairs\n\n\ndef get_image_pairs_shortlist(fnames,\n                              sim_th = 0.6, # should be strict\n                              min_pairs = 30,\n                              exhaustive_if_less = 20,\n                              device=torch.device('cpu')):\n    num_imgs = len(fnames)\n    if num_imgs <= exhaustive_if_less:\n        return get_img_pairs_exhaustive(fnames)\n    descs = torch.tensor(get_clip_embeddings(fnames, device))\n    dm = torch.cdist(descs, descs, p=2).numpy()\n    # removing half\n    mask = dm <= sim_th\n    total = 0\n    matching_list = []\n    ar = np.arange(num_imgs)\n    already_there_set = []\n    for st_idx in range(num_imgs-1):\n        mask_idx = mask[st_idx]\n        to_match = ar[mask_idx]\n        if len(to_match) < min_pairs:\n            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n        for idx in to_match:\n            if st_idx == idx:\n                continue\n            if dm[st_idx, idx] < 1000:\n                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n                total+=1\n    matching_list = sorted(list(set(matching_list)))\n    return matching_list\n\ndef detect_aliked(img_fnames,\n                  feature_dir = '.featureout',\n                  num_features = 4096,\n                  resize_to = 1024,\n                  device=torch.device('cpu')):\n    dtype = torch.float32 # ALIKED has issues with float16\n    extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.01, resize=resize_to).eval().to(device, dtype)\n    if not os.path.isdir(feature_dir):\n        os.makedirs(feature_dir)\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n        for img_path in tqdm(img_fnames):\n            img_fname = img_path.split('/')[-1]\n            key = img_fname\n            with torch.inference_mode():\n                image0 = load_torch_image(img_path, device=device).to(dtype)\n                feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n                f_kp[key] = kpts\n                f_desc[key] = descs\n    return\n\ndef match_with_lightglue(img_fnames,\n                   index_pairs,\n                   feature_dir = '.featureout',\n                   device=torch.device('cpu'),\n                   min_matches=25,verbose=True):\n    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n                                                \"depth_confidence\": -1,\n                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n        h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n        for pair_idx in tqdm(index_pairs):\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n            with torch.inference_mode():\n                dists, idxs = lg_matcher(desc1,\n                                         desc2,\n                                         KF.laf_from_center_scale_ori(kp1[None]),\n                                         KF.laf_from_center_scale_ori(kp2[None]))\n            if len(idxs)  == 0:\n                continue\n            n_matches = len(idxs)\n            if verbose:\n                print (f'{key1}-{key2}: {n_matches} matches')\n            group  = f_match.require_group(key1)\n            if n_matches >= min_matches:\n                 group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n    return\n\ndef import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n    add_matches(\n        db,\n        feature_dir,\n        fname_to_id,\n    )\n    db.commit()\n    return","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@dataclasses.dataclass\nclass Prediction:\n    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n    dataset: str\n    filename: str\n    cluster_index: int | None = None\n    rotation: np.ndarray | None = None\n    translation: np.ndarray | None = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T22:44:49.738085Z","iopub.execute_input":"2025-04-22T22:44:49.738447Z","iopub.status.idle":"2025-04-22T22:44:49.743550Z","shell.execute_reply.started":"2025-04-22T22:44:49.738421Z","shell.execute_reply":"2025-04-22T22:44:49.742469Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"is_train = True # Set to False if submitting to contest\nworkdir = '/kaggle/working/result/'\nos.makedirs(workdir, exist_ok=True)\n\nif is_train:\n    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\nelse:\n    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n\nsamples = {}\ncompetition_data = pd.read_csv(sample_submission_csv)\nfor _, row in competition_data.iterrows():\n    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n    if row.dataset not in samples:\n        samples[row.dataset] = []\n    samples[row.dataset].append(\n        Prediction(\n            image_id=None if is_train else row.image_id,\n            dataset=row.dataset,\n            filename=row.image\n        )\n    )\n\nfor dataset in samples:\n    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T22:44:51.296768Z","iopub.execute_input":"2025-04-22T22:44:51.297087Z","iopub.status.idle":"2025-04-22T22:44:51.455584Z","shell.execute_reply.started":"2025-04-22T22:44:51.297060Z","shell.execute_reply":"2025-04-22T22:44:51.454443Z"}},"outputs":[{"name":"stdout","text":"Dataset \"imc2023_haiper\" -> num_images=54\nDataset \"imc2023_heritage\" -> num_images=209\nDataset \"imc2023_theather_imc2024_church\" -> num_images=76\nDataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\nDataset \"imc2024_lizard_pond\" -> num_images=214\nDataset \"pt_brandenburg_british_buckingham\" -> num_images=225\nDataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\nDataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\nDataset \"pt_stpeters_stpauls\" -> num_images=200\nDataset \"amy_gardens\" -> num_images=200\nDataset \"fbk_vineyard\" -> num_images=163\nDataset \"ETs\" -> num_images=22\nDataset \"stairs\" -> num_images=51\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Visualize clusters","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport random\n\ndef visualize_clusters(umap_embeddings, cluster_labels, image_paths, num_samples=5):\n    \"\"\"\n    Visualizes UMAP-reduced embeddings with HDBSCAN clusters and displays example images from a random cluster.\n    \n    Args:\n        umap_embeddings (numpy.ndarray): 2D UMAP-reduced embeddings (N x 2).\n        cluster_labels (numpy.ndarray): Cluster assignments for each image (N,).\n        image_paths (list): List of file paths to the images (N,).\n        num_samples (int): Number of images to display from a selected cluster.\n    \"\"\"\n\n    unique_clusters = set(cluster_labels)\n    palette = sns.color_palette(\"husl\", len(unique_clusters))  # Generate colors for clusters\n    \n    # Scatter plot of clusters\n    plt.figure(figsize=(10, 6))\n    for cluster_id in unique_clusters:\n        indices = np.where(cluster_labels == cluster_id)[0]\n        plt.scatter(umap_embeddings[indices, 0], umap_embeddings[indices, 1], \n                    label=f\"Cluster {cluster_id}\" if cluster_id != -1 else \"Outliers\",\n                    alpha=0.6, edgecolors='k', s=30, color=palette[cluster_id % len(palette)])\n    \n    plt.xlabel(\"UMAP Dimension 1\")\n    plt.ylabel(\"UMAP Dimension 2\")\n    plt.title(\"UMAP Visualization of Image Clusters\")\n    plt.legend()\n    plt.show()\n\n    # Pick a random cluster (excluding outliers, labeled as -1)\n    valid_clusters = [c for c in unique_clusters if c != -1]\n    if not valid_clusters:\n        print(\"No valid clusters found.\")\n        return\n\n    selected_cluster = random.choice(valid_clusters)\n    print(f\"\\nDisplaying {num_samples} images from Cluster {selected_cluster}:\")\n\n    # Get image paths from the selected cluster\n    cluster_indices = np.where(cluster_labels == selected_cluster)[0]\n    sample_indices = random.sample(list(cluster_indices), min(num_samples, len(cluster_indices)))\n    sample_images = [image_paths[i] for i in sample_indices]\n\n    # Display images\n    fig, axes = plt.subplots(1, len(sample_images), figsize=(15, 5))\n    for ax, img_path in zip(axes, sample_images):\n        img = Image.open(img_path)\n        ax.imshow(img)\n        ax.axis(\"off\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clustering example on imc2023_haiper\ndatasets = [\"imc2023_haiper\"]\nimage_paths = pick_images(image_dir, datasets)\nreduced_embeddings = get_clip_embeddings(image_paths, device)\nclusters = cluster_images(reduced_embeddings)\nvisualize_clusters(reduced_embeddings, clusters, image_paths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run COLMAP for Camera Pose Estimation","metadata":{}},{"cell_type":"code","source":"datasets_to_process = None #Run on all test datasets\nif is_train:\n    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n    datasets_to_process = [\n    \t# New data.\n    \t'amy_gardens',\n    \t'ETs',\n    \t'fbk_vineyard',\n    \t'stairs',\n    \t# Data from IMC 2023 and 2024.\n    \t# 'imc2024_dioscuri_baalshamin',\n    \t# 'imc2023_theather_imc2024_church',\n    \t# 'imc2023_heritage',\n    \t# 'imc2023_haiper',\n    \t# 'imc2024_lizard_pond',\n    \t# Crowdsourced PhotoTourism data.\n    \t# 'pt_stpeters_stpauls',\n    \t#'pt_brandenburg_british_buckingham',\n    \t# 'pt_piazzasanmarco_grandplace',\n    \t# 'pt_sacrecoeur_trevi_tajmahal',\n    ]\n\ntimings = {\n    \"shortlisting\":[],\n    \"feature_detection\": [],\n    \"feature_matching\":[],\n    \"RANSAC\": [],\n    \"Reconstruction\": [],\n}\nmapping_result_strs = []\n\n\nprint (f\"Extracting on device {device}\")\nfor dataset, predictions in samples.items():\n    if datasets_to_process and dataset not in datasets_to_process:\n        print(f'Skipping \"{dataset}\"')\n        continue\n    \n    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n    images = [os.path.join(images_dir, p.filename) for p in predictions]\n\n    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n\n    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n\n    feature_dir = os.path.join(workdir, 'featureout', dataset)\n    os.makedirs(feature_dir, exist_ok=True)\n\n    # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n    try:\n        t = time()\n        index_pairs = get_image_pairs_shortlist(\n            images,\n            sim_th = 0.3, # should be strict\n            min_pairs = 20, # we should select at least min_pairs PER IMAGE with biggest similarity\n            exhaustive_if_less = 20,\n            device=device\n        )\n        timings['shortlisting'].append(time() - t)\n        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n        gc.collect()\n    \n        t = time()\n\n        detect_aliked(images, feature_dir, 4096, device=device)\n        gc.collect()\n        timings['feature_detection'].append(time() - t)\n        print(f'Features detected in {time() - t:.4f} sec')\n        \n        t = time()\n        match_with_lightglue(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n        timings['feature_matching'].append(time() - t)\n        print(f'Features matched in {time() - t:.4f} sec')\n\n        database_path = os.path.join(feature_dir, 'colmap.db')\n        if os.path.isfile(database_path):\n            os.remove(database_path)\n        gc.collect()\n        sleep(1)\n        import_into_colmap(images_dir, feature_dir=feature_dir, database_path=database_path)\n        output_path = f'{feature_dir}/colmap_rec_aliked'\n        \n        t = time()\n        pycolmap.match_exhaustive(database_path)\n        timings['RANSAC'].append(time() - t)\n        print(f'Ran RANSAC in {time() - t:.4f} sec')\n        \n        # By default colmap does not generate a reconstruction if less than 10 images are registered.\n        # Lower it to 3.\n        mapper_options = pycolmap.IncrementalPipelineOptions()\n        mapper_options.min_model_size = 3\n        mapper_options.max_num_models = 25\n        os.makedirs(output_path, exist_ok=True)\n        t = time()\n        maps = pycolmap.incremental_mapping(\n            database_path=database_path, \n            image_path=images_dir,\n            output_path=output_path,\n            options=mapper_options)\n        sleep(1)\n        timings['Reconstruction'].append(time() - t)\n        print(f'Reconstruction done in  {time() - t:.4f} sec')\n        print(maps)\n\n        clear_output(wait=False)\n    \n        registered = 0\n        for map_index, cur_map in maps.items():\n            for index, image in cur_map.images.items():\n                prediction_index = filename_to_index[image.name]\n                predictions[prediction_index].cluster_index = map_index\n                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n                registered += 1\n        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(maps)} clusters'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n        gc.collect()\n    except Exception as e:\n        print(e)\n        # raise e\n        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n\nprint('\\nResults')\nfor s in mapping_result_strs:\n    print(s)\n\nprint('\\nTimings')\nfor k, v in timings.items():\n    print(f'{k} -> total={sum(v):.02f} sec.')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run Fast3R for camera pose estimation","metadata":{}},{"cell_type":"code","source":"# Download PyTorch3D from wheel\nimport sys\nimport torch\npyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\nversion_str=\"\".join([\n    f\"py3{sys.version_info.minor}_cu\",\n    torch.version.cuda.replace(\".\",\"\"),\n    f\"_pyt{pyt_version_str}\"\n])\nprint(version_str)\n!pip install -q iopath\n!pip install -q --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# clone Fast3R repo\n!git clone https://github.com/facebookresearch/fast3r\n!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu125\n\n# install requirements\n!pip install -q -r /kaggle/working/fast3r/requirements.txt\n\n# install fast3r as a package (so you can import fast3r and use it in your own project)\n!pip install -q -e /kaggle/working/fast3r","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Debugging in case you get ModuleNotFoundError\nimport os\nimport pkgutil\nimport fast3r\nimport sys\nprint(sys.path)\nprint([p for p in sys.path if 'fast3r' in p])\nprint([k for k in sys.modules if k.startswith('fast3r')])\n\nfor sub in pkgutil.iter_modules(fast3r.__path__):\n    print(sub)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom fast3r.models.fast3r import Fast3R\nfrom fast3r.models.multiview_dust3r_module import MultiViewDUSt3RLitModule\nfrom fast3r.dust3r.utils.image import load_images\nfrom fast3r.dust3r.inference_multiview import inference\nimport numpy as np\nimport gc\n\ndef camera_pose_estimation(clusters, dataset, device):\n    predictions = samples[dataset]\n    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n    # Load the model from Hugging Face\n    model = Fast3R.from_pretrained(\"jedyang97/Fast3R_ViT_Large_512\")\n    model.encoder_args[\"attn_implementation\"] = \"pytorch_naive\"\n    model.build_encoder(model.encoder_args)\n    model.decoder_args[\"attn_implementation\"] = \"pytorch_naive\"\n    model.build_decoder(model.decoder_args)\n    \n    model = model.to(device)\n    lit_module = MultiViewDUSt3RLitModule.load_for_inference(model)\n    \n    model.eval()\n    lit_module.eval()\n    for cluster in clusters:\n        if cluster == \"outliers\":\n            # Skip inference for outliers\n            for outlier in clusters[cluster]:\n                prediction_index = filename_to_index[os.path.basename(outlier)]\n                predictions[prediction_index].cluster_index = None\n            continue\n            \n        # Load images\n        images = load_images(clusters[cluster], size=384, verbose=False)\n    \n        # Run inference\n        try:\n            output_dict = inference(\n                images,\n                model,\n                device,\n                dtype=torch.float32,\n                verbose=False,\n                profiling=False\n            )\n        except Exception as e:\n            print(e)\n            torch.cuda.empty_cache()\n            gc.collect()\n            continue\n        \n        # Extract camera poses\n        poses_c2w_batch, estimated_focals = MultiViewDUSt3RLitModule.estimate_camera_poses(\n            output_dict['preds'],\n            niter_PnP=100,\n            focal_length_estimation_method='first_view_from_global_head'\n        )\n        # poses_c2w_batch is a list; the first element contains the estimated poses for each view.\n        camera_poses = poses_c2w_batch[0]\n\n        # Record rotation and translation\n        for pose, image_path in zip(camera_poses, clusters[cluster]):\n            prediction_index = filename_to_index[os.path.basename(image_path)]\n    \n            predictions[prediction_index].cluster_index = cluster\n            predictions[prediction_index].rotation = deepcopy(pose[:3, :3])\n            predictions[prediction_index].translation = deepcopy(pose[:3, 3])\n    \n        # Release GPU memory when done\n        del poses_c2w_batch\n        torch.cuda.empty_cache()\n        gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T23:14:01.411251Z","iopub.execute_input":"2025-04-22T23:14:01.411758Z","iopub.status.idle":"2025-04-22T23:14:01.422080Z","shell.execute_reply.started":"2025-04-22T23:14:01.411716Z","shell.execute_reply":"2025-04-22T23:14:01.420966Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Evaluation on clusters found using CLIP ViT encoder\n\n#datasets = list(train_labels[\"dataset\"].unique()) # All datasets in train dir\ndatasets = [\"ETs\"]\ndata = []\nimages_dir = os.path.join(data_dir, 'train' if is_train else 'test')\nfor dataset in datasets:\n    image_paths = pick_images(images_dir, datasets=[dataset])\n    print(f\"Dataset: {dataset} with {len(image_paths)} images.\")\n    print(\"Clustering images...\")\n    reduced_embeddings = get_clip_embeddings(image_paths, device)\n    clusters = cluster_images(reduced_embeddings)\n    clusters_dict = {} \n    for path, cluster_name in zip(image_paths, clusters):\n        if cluster_name == -1:\n            cluster_name = \"outliers\"\n        if cluster_name not in clusters_dict:\n            clusters_dict[cluster_name] = []\n        clusters_dict[cluster_name].append(path)\n    print(f\"Found {len(clusters_dict)} clusters.\")\n\n    print(\"Estimating camera poses for each cluster...\")\n    camera_pose_estimation(clusters_dict, dataset, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T23:14:03.851023Z","iopub.execute_input":"2025-04-22T23:14:03.851399Z","iopub.status.idle":"2025-04-22T23:14:26.441508Z","shell.execute_reply.started":"2025-04-22T23:14:03.851371Z","shell.execute_reply":"2025-04-22T23:14:26.440734Z"}},"outputs":[{"name":"stdout","text":"Dataset: ETs with 22 images.\nClustering images...\nFound 2 clusters.\nEstimating camera poses for each cluster...\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"## Save data to CSV and Evaluate","metadata":{}},{"cell_type":"code","source":"# Must Create a submission file.\n\narray_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\nnone_to_str = lambda n: ';'.join(['nan'] * n)\n\nsubmission_file = '/kaggle/working/submission.csv'\nwith open(submission_file, 'w') as f:\n    if is_train:\n        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n    else:\n        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n\n!head {submission_file}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T23:16:38.382388Z","iopub.execute_input":"2025-04-22T23:16:38.382771Z","iopub.status.idle":"2025-04-22T23:16:38.652476Z","shell.execute_reply.started":"2025-04-22T23:16:38.382746Z","shell.execute_reply":"2025-04-22T23:16:38.651522Z"}},"outputs":[{"name":"stdout","text":"dataset,scene,image,rotation_matrix,translation_vector\nimc2023_haiper,outliers,fountain_image_116.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_108.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_101.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_082.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_071.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_025.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_000.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_007.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_012.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"if is_train:\n    t = time()\n    final_score, dataset_scores = metric.score(\n        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n        user_csv=submission_file,\n        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n        inl_cf=0,\n        strict_cf=-1,\n        verbose=True,\n    )\n    print(f'Computed metric in: {time() - t:.02f} sec.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T23:16:41.007460Z","iopub.execute_input":"2025-04-22T23:16:41.007836Z","iopub.status.idle":"2025-04-22T23:18:48.025735Z","shell.execute_reply.started":"2025-04-22T23:16:41.007806Z","shell.execute_reply":"2025-04-22T23:18:48.024956Z"}},"outputs":[{"name":"stdout","text":"imc2023_haiper: score=0.00% (mAA=0.00%, clusterness=0.00%)\nimc2023_heritage: score=0.00% (mAA=0.00%, clusterness=0.00%)\nimc2023_theather_imc2024_church: score=0.00% (mAA=0.00%, clusterness=0.00%)\nimc2024_dioscuri_baalshamin: score=0.00% (mAA=0.00%, clusterness=0.00%)\nimc2024_lizard_pond: score=0.00% (mAA=0.00%, clusterness=0.00%)\npt_brandenburg_british_buckingham: score=0.00% (mAA=0.00%, clusterness=0.00%)\npt_piazzasanmarco_grandplace: score=0.00% (mAA=0.00%, clusterness=0.00%)\npt_sacrecoeur_trevi_tajmahal: score=0.00% (mAA=0.00%, clusterness=0.00%)\npt_stpeters_stpauls: score=0.00% (mAA=0.00%, clusterness=0.00%)\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/input/imc25-utils/metric.py:293: RuntimeWarning: divide by zero encountered in scalar divide\n  M[:ndims, :ndims] *= math.sqrt(np.sum(v1) / np.sum(v0))\n/kaggle/input/imc25-utils/metric.py:293: RuntimeWarning: invalid value encountered in multiply\n  M[:ndims, :ndims] *= math.sqrt(np.sum(v1) / np.sum(v0))\n/kaggle/input/imc25-utils/metric.py:175: RuntimeWarning: invalid value encountered in less\n  inl = np.expand_dims(err, axis=1) < ransac_threshold2\n","output_type":"stream"},{"name":"stdout","text":"amy_gardens: score=1.51% (mAA=0.76%, clusterness=100.00%)\nfbk_vineyard: score=0.00% (mAA=0.00%, clusterness=0.00%)\nETs: score=0.00% (mAA=0.00%, clusterness=86.36%)\nstairs: score=0.00% (mAA=0.00%, clusterness=0.00%)\nAverage over all datasets: score=0.12% (mAA=0.06%, clusterness=14.34%)\nComputed metric in: 127.01 sec.\n","output_type":"stream"}],"execution_count":36}]}