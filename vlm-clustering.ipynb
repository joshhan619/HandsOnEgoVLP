{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91498,"databundleVersionId":11655853,"sourceType":"competition"},{"sourceId":9818953,"sourceType":"datasetVersion","datasetId":2456003}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndata_dir = \"/kaggle/input/image-matching-challenge-2025\"\ntrain_labels = pd.read_csv(\"/kaggle/input/image-matching-challenge-2025/train_labels.csv\")\ntrain_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:02:54.199459Z","iopub.execute_input":"2025-04-16T17:02:54.199778Z","iopub.status.idle":"2025-04-16T17:02:54.524735Z","shell.execute_reply.started":"2025-04-16T17:02:54.199749Z","shell.execute_reply":"2025-04-16T17:02:54.524053Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"             dataset           scene                             image  \\\n0     imc2023_haiper        fountain            fountain_image_116.png   \n1     imc2023_haiper        fountain            fountain_image_108.png   \n2     imc2023_haiper        fountain            fountain_image_101.png   \n3     imc2023_haiper        fountain            fountain_image_082.png   \n4     imc2023_haiper        fountain            fountain_image_071.png   \n...              ...             ...                               ...   \n1940          stairs  stairs_split_2  stairs_split_2_1710453733751.png   \n1941          stairs  stairs_split_2  stairs_split_2_1710453759963.png   \n1942          stairs  stairs_split_2  stairs_split_2_1710453805788.png   \n1943          stairs  stairs_split_2  stairs_split_2_1710453765165.png   \n1944          stairs  stairs_split_2  stairs_split_2_1710453774370.png   \n\n                                        rotation_matrix  \\\n0     0.122655949;0.947713775;-0.294608417;0.1226706...   \n1     0.474305910;0.359108654;-0.803787832;0.2888416...   \n2     0.565115476;-0.138485064;-0.813305838;0.506678...   \n3     -0.308320392;-0.794654112;0.522937261;0.948141...   \n4     -0.569002830;-0.103808175;0.815757098;0.778745...   \n...                                                 ...   \n1940  0.961762441;-0.187990401;0.199179859;-0.177691...   \n1941  0.237960308;0.580896704;-0.778417569;0.4077886...   \n1942  0.309067298;0.541767194;-0.781642957;0.4038963...   \n1943  0.301920210;0.609614467;-0.732949103;0.5007116...   \n1944  0.744374958;0.439048204;-0.503132782;0.4197719...   \n\n                          translation_vector  \n0       0.093771314;-0.803560988;2.062001533  \n1       0.358946647;-0.797557548;1.910906929  \n2       0.146922468;-0.981392596;2.009002852  \n3       0.206413831;-1.174321103;3.667167680  \n4      -0.015140892;-1.334052012;3.488936597  \n...                                      ...  \n1940  -0.112850000;-3.521750000;-2.859750000  \n1941   -0.490768000;-3.064140000;3.008420000  \n1942    -0.572757000;0.885835000;4.987270000  \n1943   -0.135613000;-1.832910000;1.598790000  \n1944   -0.367979000;-0.607935000;0.621483000  \n\n[1945 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>scene</th>\n      <th>image</th>\n      <th>rotation_matrix</th>\n      <th>translation_vector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>imc2023_haiper</td>\n      <td>fountain</td>\n      <td>fountain_image_116.png</td>\n      <td>0.122655949;0.947713775;-0.294608417;0.1226706...</td>\n      <td>0.093771314;-0.803560988;2.062001533</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>imc2023_haiper</td>\n      <td>fountain</td>\n      <td>fountain_image_108.png</td>\n      <td>0.474305910;0.359108654;-0.803787832;0.2888416...</td>\n      <td>0.358946647;-0.797557548;1.910906929</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>imc2023_haiper</td>\n      <td>fountain</td>\n      <td>fountain_image_101.png</td>\n      <td>0.565115476;-0.138485064;-0.813305838;0.506678...</td>\n      <td>0.146922468;-0.981392596;2.009002852</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>imc2023_haiper</td>\n      <td>fountain</td>\n      <td>fountain_image_082.png</td>\n      <td>-0.308320392;-0.794654112;0.522937261;0.948141...</td>\n      <td>0.206413831;-1.174321103;3.667167680</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>imc2023_haiper</td>\n      <td>fountain</td>\n      <td>fountain_image_071.png</td>\n      <td>-0.569002830;-0.103808175;0.815757098;0.778745...</td>\n      <td>-0.015140892;-1.334052012;3.488936597</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1940</th>\n      <td>stairs</td>\n      <td>stairs_split_2</td>\n      <td>stairs_split_2_1710453733751.png</td>\n      <td>0.961762441;-0.187990401;0.199179859;-0.177691...</td>\n      <td>-0.112850000;-3.521750000;-2.859750000</td>\n    </tr>\n    <tr>\n      <th>1941</th>\n      <td>stairs</td>\n      <td>stairs_split_2</td>\n      <td>stairs_split_2_1710453759963.png</td>\n      <td>0.237960308;0.580896704;-0.778417569;0.4077886...</td>\n      <td>-0.490768000;-3.064140000;3.008420000</td>\n    </tr>\n    <tr>\n      <th>1942</th>\n      <td>stairs</td>\n      <td>stairs_split_2</td>\n      <td>stairs_split_2_1710453805788.png</td>\n      <td>0.309067298;0.541767194;-0.781642957;0.4038963...</td>\n      <td>-0.572757000;0.885835000;4.987270000</td>\n    </tr>\n    <tr>\n      <th>1943</th>\n      <td>stairs</td>\n      <td>stairs_split_2</td>\n      <td>stairs_split_2_1710453765165.png</td>\n      <td>0.301920210;0.609614467;-0.732949103;0.5007116...</td>\n      <td>-0.135613000;-1.832910000;1.598790000</td>\n    </tr>\n    <tr>\n      <th>1944</th>\n      <td>stairs</td>\n      <td>stairs_split_2</td>\n      <td>stairs_split_2_1710453774370.png</td>\n      <td>0.744374958;0.439048204;-0.503132782;0.4197719...</td>\n      <td>-0.367979000;-0.607935000;0.621483000</td>\n    </tr>\n  </tbody>\n</table>\n<p>1945 rows Ã— 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"!pip install umap-learn\n!pip install hdbscan\n!pip install pycolmap","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\ndef pick_images(image_dir, datasets=[]):\n    image_paths = []\n    if len(datasets) == 0:\n        df = train_labels\n    else:\n        df = train_labels[train_labels[\"dataset\"].isin(datasets)]\n    for _, row in df.iterrows():\n        path = os.path.join(image_dir, row[\"dataset\"], row[\"image\"])\n        image_paths.append(path)\n    return image_paths\n\nimage_dir = f\"{data_dir}/train\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:02:58.060840Z","iopub.execute_input":"2025-04-16T17:02:58.061171Z","iopub.status.idle":"2025-04-16T17:02:59.721335Z","shell.execute_reply.started":"2025-04-16T17:02:58.061143Z","shell.execute_reply":"2025-04-16T17:02:59.720599Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\nimport os\nimport cv2 as cv\nimport torch\nimport umap\nfrom hdbscan import HDBSCAN\nimport gc\n    \n\ndef cluster_images(image_paths, device):\n    # Load Pretrained CLIP\n    model_name = \"openai/clip-vit-base-patch32\"\n    clip_model = CLIPModel.from_pretrained(model_name).to(device)\n    processor = CLIPProcessor.from_pretrained(model_name)\n\n    def get_image_embeddings(image_path: str):\n        image = Image.open(image_path)\n    \n        with torch.no_grad():\n            inputs = processor(images=image, return_tensors=\"pt\", padding=True).to(device)\n            return clip_model.get_image_features(**inputs)\n\n    # Get image embeddings using CLIP\n    image_embeddings = torch.cat([get_image_embeddings(image_path) for image_path in image_paths])\n\n    # Dimensionality reduction\n    umap_model = umap.UMAP(n_components = 5)\n    reduced_embeddings = umap_model.fit_transform(image_embeddings.cpu().numpy())\n\n    # Clustering\n    clusterer = HDBSCAN(min_cluster_size=3)\n    clusterer.fit(reduced_embeddings)\n\n    clusters = clusterer.labels_\n\n    # Cleanup\n    del clip_model\n    del image_embeddings\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return reduced_embeddings, clusters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:02:59.722481Z","iopub.execute_input":"2025-04-16T17:02:59.723034Z","iopub.status.idle":"2025-04-16T17:03:09.132868Z","shell.execute_reply.started":"2025-04-16T17:02:59.723006Z","shell.execute_reply":"2025-04-16T17:03:09.131863Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Visualize clusters","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport random\n\ndef visualize_clusters(umap_embeddings, cluster_labels, image_paths, num_samples=5):\n    \"\"\"\n    Visualizes UMAP-reduced embeddings with HDBSCAN clusters and displays example images from a random cluster.\n    \n    Args:\n        umap_embeddings (numpy.ndarray): 2D UMAP-reduced embeddings (N x 2).\n        cluster_labels (numpy.ndarray): Cluster assignments for each image (N,).\n        image_paths (list): List of file paths to the images (N,).\n        num_samples (int): Number of images to display from a selected cluster.\n    \"\"\"\n\n    unique_clusters = set(cluster_labels)\n    palette = sns.color_palette(\"husl\", len(unique_clusters))  # Generate colors for clusters\n    \n    # Scatter plot of clusters\n    plt.figure(figsize=(10, 6))\n    for cluster_id in unique_clusters:\n        indices = np.where(cluster_labels == cluster_id)[0]\n        plt.scatter(umap_embeddings[indices, 0], umap_embeddings[indices, 1], \n                    label=f\"Cluster {cluster_id}\" if cluster_id != -1 else \"Outliers\",\n                    alpha=0.6, edgecolors='k', s=30, color=palette[cluster_id % len(palette)])\n    \n    plt.xlabel(\"UMAP Dimension 1\")\n    plt.ylabel(\"UMAP Dimension 2\")\n    plt.title(\"UMAP Visualization of Image Clusters\")\n    plt.legend()\n    plt.show()\n\n    # Pick a random cluster (excluding outliers, labeled as -1)\n    valid_clusters = [c for c in unique_clusters if c != -1]\n    if not valid_clusters:\n        print(\"No valid clusters found.\")\n        return\n\n    selected_cluster = random.choice(valid_clusters)\n    print(f\"\\nDisplaying {num_samples} images from Cluster {selected_cluster}:\")\n\n    # Get image paths from the selected cluster\n    cluster_indices = np.where(cluster_labels == selected_cluster)[0]\n    sample_indices = random.sample(list(cluster_indices), min(num_samples, len(cluster_indices)))\n    sample_images = [image_paths[i] for i in sample_indices]\n\n    # Display images\n    fig, axes = plt.subplots(1, len(sample_images), figsize=(15, 5))\n    for ax, img_path in zip(axes, sample_images):\n        img = Image.open(img_path)\n        ax.imshow(img)\n        ax.axis(\"off\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clustering example on imc2023_haiper\ndatasets = [\"stairs\"]\nimage_paths = pick_images(image_dir, datasets)\nreduced_embeddings, clusters = cluster_images(image_paths, device)\nvisualize_clusters(reduced_embeddings, clusters, image_paths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run COLMAP for Camera Pose Estimation","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport pycolmap\nfrom tqdm import tqdm\n\ndef run_colmap_on_cluster(image_paths, output_dir, do_extract=True):\n    \"\"\"Runs COLMAP on a subset of images and returns poses.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    database_path = os.path.join(output_dir, \"database.db\")\n    image_dir = os.path.dirname(image_paths[0])\n\n    if do_extract:\n        image_list = [os.path.basename(img) for img in image_paths]\n        pycolmap.extract_features(\n            database_path=database_path,\n            image_path=image_dir,\n            image_list=image_list,\n            camera_mode=pycolmap.CameraMode.AUTO,\n            device=pycolmap.Device('auto'),\n            sift_options=pycolmap.SiftExtractionOptions(estimate_affine_shape=True),\n        )\n\n        pycolmap.match_exhaustive(database_path, device=pycolmap.Device('auto'))\n\n    reconstructions = pycolmap.incremental_mapping(\n        database_path=database_path,\n        image_path=image_dir,\n        output_path=output_dir,\n    )\n\n    # Extract poses (R, T) for each image\n    poses = {}\n    if reconstructions:\n        reconstruction = next(iter(reconstructions.values()))\n        for image_id, image in reconstruction.images.items():\n            img_name = image.name\n            rigid3d = image.cam_from_world\n            R = rigid3d.rotation.matrix().ravel()\n            R = \";\".join(map(str, list(R)))\n            T = rigid3d.translation\n            T = \";\".join(map(str, list(T)))\n            poses[img_name] = (R, T)\n    return poses\n\ndef process_clusters(image_paths, cluster_labels, output_dir=\"colmap_temp\", do_extract=True):\n    \"\"\"Processes each cluster with COLMAP and compiles results.\"\"\"\n    results = []\n\n    # Group images by cluster\n    clusters = {}\n    for idx, label in enumerate(cluster_labels):\n        if label not in clusters:\n            clusters[label] = []\n        clusters[label].append(image_paths[idx])\n\n    # Process each cluster\n    for cluster_id, cluster_images in tqdm(clusters.items()):\n        if cluster_id == -1:\n            # Outliers: No pose estimation\n            for img in cluster_images:\n                dataset = img.split(os.path.sep)[-2]\n                results.append({\n                    \"dataset\": dataset,\n                    \"scene\": \"outliers\",\n                    \"image\": os.path.basename(img),\n                    \"rotation_matrix\": \";\".join(map(str, [np.nan] * 9)),\n                    \"translation_vector\": \";\".join(map(str, [np.nan] * 3)),\n                })\n        else:\n            # Run COLMAP on the cluster\n            poses = run_colmap_on_cluster(cluster_images, f\"{output_dir}/cluster_{cluster_id}\", do_extract)\n\n            # Assign results\n            for img in cluster_images:\n                img_name = os.path.basename(img)\n                dataset = img.split(os.path.sep)[-2]\n                if img_name in poses:\n                    R, T = poses[img_name]\n                    results.append({\n                        \"dataset\": dataset,\n                        \"scene\": f\"cluster_{cluster_id}\",\n                        \"image\": img_name,\n                        \"rotation_matrix\": R,\n                        \"translation_vector\": T,\n                    })\n                else:\n                    # COLMAP failed to register this image\n                    results.append({\n                        \"dataset\": dataset,\n                        \"scene\": f\"cluster_{cluster_id}\",\n                        \"image\": img_name,\n                        \"rotation_matrix\": \";\".join(map(str, [np.nan] * 9)),\n                        \"translation_vector\": \";\".join(map(str, [np.nan] * 3)),\n                    })\n    return pd.DataFrame(results)\n\n\ndf_submission = process_clusters(image_paths, clusters, do_extract=False)\ndf_submission","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run Fast3R for camera pose estimation","metadata":{}},{"cell_type":"code","source":"# Download PyTorch3D from wheel\nimport sys\nimport torch\npyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\nversion_str=\"\".join([\n    f\"py3{sys.version_info.minor}_cu\",\n    torch.version.cuda.replace(\".\",\"\"),\n    f\"_pyt{pyt_version_str}\"\n])\nprint(version_str)\n!pip install -q iopath\n!pip install -q --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# clone Fast3R repo\n!git clone https://github.com/facebookresearch/fast3r\n!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu125\n\n# install requirements\n!pip install -q -r /kaggle/working/fast3r/requirements.txt\n\n# install fast3r as a package (so you can import fast3r and use it in your own project)\n!pip install -q -e /kaggle/working/fast3r","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Restart to use fast3r module\n%load_ext autoreload\n%autoreload 2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Debugging in case you get ModuleNotFoundError\nimport os\nimport pkgutil\nimport fast3r\nimport sys\nprint(sys.path)\nprint([p for p in sys.path if 'fast3r' in p])\nprint([k for k in sys.modules if k.startswith('fast3r')])\n\nfor sub in pkgutil.iter_modules(fast3r.__path__):\n    print(sub)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef pose_to_data(camera_poses, cluster, cluster_name, dataset):\n    data = []\n    for pose, image_path in zip(camera_poses, cluster):\n        rotation_matrix = pose[:3, :3].flatten()  # shape (9,)\n        translation_vector = pose[:3, 3].flatten()  # shape (3,)\n        \n        rotation_str = \";\".join(str(x) for x in rotation_matrix)\n        translation_str = \";\".join(str(x) for x in translation_vector)\n        \n        data.append({\n            \"dataset\": dataset,\n            \"scene\": cluster_name,\n            \"image\": os.path.basename(image_path),\n            \"rotation_matrix\": rotation_str,\n            \"translation_vector\": translation_str\n        })\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:03:15.521661Z","iopub.execute_input":"2025-04-16T17:03:15.522526Z","iopub.status.idle":"2025-04-16T17:03:15.529342Z","shell.execute_reply.started":"2025-04-16T17:03:15.522479Z","shell.execute_reply":"2025-04-16T17:03:15.528417Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom fast3r.models.fast3r import Fast3R\nfrom fast3r.models.multiview_dust3r_module import MultiViewDUSt3RLitModule\nfrom fast3r.dust3r.utils.image import load_images\nfrom fast3r.dust3r.inference_multiview import inference\nimport numpy as np\nimport gc\n\ndef camera_pose_estimation(clusters, dataset, device):\n    data = []\n    # Load the model from Hugging Face\n    model = Fast3R.from_pretrained(\"jedyang97/Fast3R_ViT_Large_512\")\n    model.encoder_args[\"attn_implementation\"] = \"pytorch_naive\"\n    model.build_encoder(model.encoder_args)\n    model.decoder_args[\"attn_implementation\"] = \"pytorch_naive\"\n    model.build_decoder(model.decoder_args)\n    \n    model = model.to(device)\n    lit_module = MultiViewDUSt3RLitModule.load_for_inference(model)\n    \n    model.eval()\n    lit_module.eval()\n    for cluster in clusters:\n        if cluster == \"outliers\":\n            # Skip inference for outliers\n            for outlier in clusters[cluster]:\n                data.append({\n                    \"dataset\": dataset,\n                    \"scene\": cluster,\n                    \"image\": os.path.basename(outlier),\n                    \"rotation_matrix\": \";\".join(map(str, [np.nan] * 9)),\n                    \"translation_vector\": \";\".join(map(str, [np.nan] * 3))\n                })\n            continue\n            \n        # Load images\n        images = load_images(clusters[cluster], size=64, verbose=False)\n    \n        # Run inference\n        try:\n            output_dict, profiling_info = inference(\n                images,\n                model,\n                device,\n                dtype=torch.float16, # Play around with this. original value is float32\n                verbose=False,\n                profiling=True\n            )\n        except Exception as e:\n            print(e)\n            torch.cuda.empty_cache()\n            gc.collect()\n            continue\n        \n        # Extract camera poses\n        poses_c2w_batch, estimated_focals = MultiViewDUSt3RLitModule.estimate_camera_poses(\n            output_dict['preds'],\n            niter_PnP=100,\n            focal_length_estimation_method='first_view_from_global_head'\n        )\n        # poses_c2w_batch is a list; the first element contains the estimated poses for each view.\n        camera_poses = poses_c2w_batch[0]\n\n        # Record rotation and translation\n        data.extend(pose_to_data(camera_poses, clusters[cluster], cluster, dataset))\n    \n        # Release GPU memory when done\n        del poses_c2w_batch\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:38:48.909609Z","iopub.execute_input":"2025-04-16T17:38:48.910075Z","iopub.status.idle":"2025-04-16T17:38:48.919692Z","shell.execute_reply.started":"2025-04-16T17:38:48.910044Z","shell.execute_reply":"2025-04-16T17:38:48.918842Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Evaluation on clusters found using CLIP ViT encoder\nimport pandas as pd\nimport numpy as np\n\ndatasets = list(train_labels[\"dataset\"].unique()) # All datasets in train dir\ndata = []\nfor dataset in datasets:\n    image_paths = pick_images(image_dir, datasets=[dataset])\n    print(f\"Dataset: {dataset} with {len(image_paths)} images.\")\n    print(\"Clustering images...\")\n    _, clusters = cluster_images(image_paths, device)\n    clusters_dict = {} \n    for path, cluster_name in zip(image_paths, clusters):\n        if cluster_name == -1:\n            cluster_name = \"outliers\"\n        if cluster_name not in clusters_dict:\n            clusters_dict[cluster_name] = []\n        clusters_dict[cluster_name].append(path)\n    print(f\"Found {len(clusters_dict)} clusters.\")\n\n    print(\"Estimating camera poses for each cluster...\")\n    data.extend(camera_pose_estimation(clusters_dict, dataset, device))\ndf_submission = pd.DataFrame(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:38:52.024259Z","iopub.execute_input":"2025-04-16T17:38:52.024551Z","iopub.status.idle":"2025-04-16T17:49:01.746172Z","shell.execute_reply.started":"2025-04-16T17:38:52.024528Z","shell.execute_reply":"2025-04-16T17:49:01.745446Z"}},"outputs":[{"name":"stdout","text":"Dataset: imc2023_haiper with 54 images.\nClustering images...\nFound 3 clusters.\nEstimating camera poses for each cluster...\nencode_images time: 0.06632351875305176\npos emb time: 0.0007040500640869141\ndecoder time: 0.042017221450805664\nhead prepare input time: 0.0006487369537353516\nhead forward time: 0.024832487106323242\ntotal Fast3R forward time: 0.13488388061523438\nencode_images time: 0.06144404411315918\npos emb time: 0.0005242824554443359\ndecoder time: 0.03298807144165039\nhead prepare input time: 0.0004923343658447266\nhead forward time: 0.02007603645324707\ntotal Fast3R forward time: 0.11580419540405273\nencode_images time: 0.0626368522644043\npos emb time: 0.0005221366882324219\ndecoder time: 0.03606057167053223\nhead prepare input time: 0.0005033016204833984\nhead forward time: 0.02112555503845215\ntotal Fast3R forward time: 0.12113189697265625\nDataset: imc2023_heritage with 209 images.\nClustering images...\nFound 13 clusters.\nEstimating camera poses for each cluster...\nencode_images time: 0.07427167892456055\npos emb time: 0.0006737709045410156\ndecoder time: 0.05903148651123047\nhead prepare input time: 0.0006091594696044922\nhead forward time: 0.035590410232543945\ntotal Fast3R forward time: 0.17048168182373047\nencode_images time: 0.05678892135620117\npos emb time: 0.0005793571472167969\ndecoder time: 0.03196287155151367\nhead prepare input time: 0.0006198883056640625\nhead forward time: 0.04304075241088867\ntotal Fast3R forward time: 0.13335108757019043\nencode_images time: 0.060029029846191406\npos emb time: 0.0005180835723876953\ndecoder time: 0.034520864486694336\nhead prepare input time: 0.0006401538848876953\nhead forward time: 0.04903411865234375\ntotal Fast3R forward time: 0.14507722854614258\nencode_images time: 0.05765414237976074\npos emb time: 0.0005829334259033203\ndecoder time: 0.031374216079711914\nhead prepare input time: 0.0006241798400878906\nhead forward time: 0.042224884033203125\ntotal Fast3R forward time: 0.13308453559875488\nencode_images time: 0.05767416954040527\npos emb time: 0.0005006790161132812\ndecoder time: 0.03164553642272949\nhead prepare input time: 0.0005209445953369141\nhead forward time: 0.01562190055847168\ntotal Fast3R forward time: 0.10624074935913086\nencode_images time: 0.057817935943603516\npos emb time: 0.0004305839538574219\ndecoder time: 0.0319972038269043\nhead prepare input time: 0.0004985332489013672\nhead forward time: 0.014308452606201172\ntotal Fast3R forward time: 0.10536694526672363\nencode_images time: 0.607900857925415\npos emb time: 0.0005733966827392578\ndecoder time: 0.032433271408081055\nhead prepare input time: 0.0008149147033691406\nhead forward time: 0.18597793579101562\ntotal Fast3R forward time: 0.8283240795135498\nencode_images time: 1.7399542331695557\npos emb time: 0.0005908012390136719\ndecoder time: 0.061190128326416016\nhead prepare input time: 0.0017180442810058594\nhead forward time: 0.38832569122314453\ntotal Fast3R forward time: 2.192169666290283\nencode_images time: 0.7204654216766357\npos emb time: 0.0006928443908691406\ndecoder time: 0.03337717056274414\nhead prepare input time: 0.0008873939514160156\nhead forward time: 0.1503894329071045\ntotal Fast3R forward time: 0.906174898147583\nencode_images time: 0.3894624710083008\npos emb time: 0.0006005764007568359\ndecoder time: 0.03315234184265137\nhead prepare input time: 0.00069427490234375\nhead forward time: 0.08497929573059082\ntotal Fast3R forward time: 0.5092287063598633\nencode_images time: 0.40445995330810547\npos emb time: 0.0006496906280517578\ndecoder time: 0.033945560455322266\nhead prepare input time: 0.0007584095001220703\nhead forward time: 0.08338046073913574\ntotal Fast3R forward time: 0.5235841274261475\nencode_images time: 0.059385061264038086\npos emb time: 0.0006530284881591797\ndecoder time: 0.03236055374145508\nhead prepare input time: 0.0007042884826660156\nhead forward time: 0.04427671432495117\ntotal Fast3R forward time: 0.13776659965515137\nDataset: imc2023_theather_imc2024_church with 76 images.\nClustering images...\nFound 2 clusters.\nEstimating camera poses for each cluster...\nencode_images time: 2.085543394088745\npos emb time: 0.0007388591766357422\ndecoder time: 0.07131481170654297\nhead prepare input time: 0.001939535140991211\nhead forward time: 0.4524052143096924\ntotal Fast3R forward time: 2.612327814102173\nencode_images time: 1.0913727283477783\npos emb time: 0.0005552768707275391\ndecoder time: 0.04046750068664551\nhead prepare input time: 0.0012750625610351562\nhead forward time: 0.23399758338928223\ntotal Fast3R forward time: 1.3681342601776123\nDataset: imc2024_dioscuri_baalshamin with 138 images.\nClustering images...\nFound 5 clusters.\nEstimating camera poses for each cluster...\nencode_images time: 2.0534603595733643\npos emb time: 0.0006234645843505859\ndecoder time: 0.0693509578704834\nhead prepare input time: 0.0018167495727539062\nhead forward time: 0.45722126960754395\ntotal Fast3R forward time: 2.582913875579834\nencode_images time: 0.6657583713531494\npos emb time: 0.0005421638488769531\ndecoder time: 0.03581666946411133\nhead prepare input time: 0.0007777214050292969\nhead forward time: 0.13869786262512207\ntotal Fast3R forward time: 0.8419058322906494\nencode_images time: 0.39325571060180664\npos emb time: 0.0004923343658447266\ndecoder time: 0.032135009765625\nhead prepare input time: 0.0005803108215332031\nhead forward time: 0.08310174942016602\ntotal Fast3R forward time: 0.5098545551300049\nencode_images time: 0.43569421768188477\npos emb time: 0.00013518333435058594\ndecoder time: 0.033191680908203125\nhead prepare input time: 0.0006310939788818359\nhead forward time: 0.08992624282836914\ntotal Fast3R forward time: 0.5603790283203125\nencode_images time: 2.291473627090454\npos emb time: 0.000629425048828125\ndecoder time: 0.07289266586303711\nhead prepare input time: 0.0019278526306152344\nhead forward time: 0.4833035469055176\ntotal Fast3R forward time: 2.8505523204803467\nDataset: imc2024_lizard_pond with 214 images.\nClustering images...\nFound 10 clusters.\nEstimating camera poses for each cluster...\nencode_images time: 0.06738114356994629\npos emb time: 0.0002243518829345703\ndecoder time: 0.04149508476257324\nhead prepare input time: 0.0007064342498779297\nhead forward time: 0.04446816444396973\ntotal Fast3R forward time: 0.15522408485412598\nencode_images time: 0.42401647567749023\npos emb time: 0.0004775524139404297\ndecoder time: 0.03238201141357422\nhead prepare input time: 0.0006248950958251953\nhead forward time: 0.08981442451477051\ntotal Fast3R forward time: 0.5476086139678955\nencode_images time: 0.0666501522064209\npos emb time: 0.0005614757537841797\ndecoder time: 0.04383587837219238\nhead prepare input time: 0.0005815029144287109\nhead forward time: 0.08382630348205566\ntotal Fast3R forward time: 0.1957533359527588\nencode_images time: 0.058251142501831055\npos emb time: 0.0004818439483642578\ndecoder time: 0.031560420989990234\nhead prepare input time: 0.0005137920379638672\nhead forward time: 0.04483294486999512\ntotal Fast3R forward time: 0.1359102725982666\nencode_images time: 0.05802607536315918\npos emb time: 0.0004177093505859375\ndecoder time: 0.031652212142944336\nhead prepare input time: 0.0005071163177490234\nhead forward time: 0.04339241981506348\ntotal Fast3R forward time: 0.1342606544494629\nencode_images time: 0.05823469161987305\npos emb time: 0.00010323524475097656\ndecoder time: 0.03167271614074707\nhead prepare input time: 0.0005173683166503906\nhead forward time: 0.03694725036621094\ntotal Fast3R forward time: 0.12774324417114258\nencode_images time: 0.21988677978515625\npos emb time: 0.0003936290740966797\ndecoder time: 0.03333020210266113\nhead prepare input time: 0.0004894733428955078\nhead forward time: 0.04807734489440918\ntotal Fast3R forward time: 0.3025052547454834\nencode_images time: 0.06547951698303223\npos emb time: 0.0005559921264648438\ndecoder time: 0.0420994758605957\nhead prepare input time: 0.0005562305450439453\nhead forward time: 0.050757646560668945\ntotal Fast3R forward time: 0.15973925590515137\nencode_images time: 3.508253574371338\npos emb time: 0.0007193088531494141\ndecoder time: 0.10501933097839355\nhead prepare input time: 0.002788066864013672\nhead forward time: 0.7244596481323242\ntotal Fast3R forward time: 4.341556787490845\nDataset: pt_brandenburg_british_buckingham with 225 images.\nClustering images...\nFound 3 clusters.\nEstimating camera poses for each cluster...\nencode_images time: 3.1348013877868652\npos emb time: 0.0006566047668457031\ndecoder time: 0.10199999809265137\nhead prepare input time: 0.0025892257690429688\nhead forward time: 0.6693012714385986\ntotal Fast3R forward time: 3.909684181213379\nencode_images time: 3.201073408126831\npos emb time: 0.0006356239318847656\ndecoder time: 0.10350298881530762\nhead prepare input time: 0.00252532958984375\nhead forward time: 0.6737494468688965\ntotal Fast3R forward time: 3.981904983520508\nencode_images time: 3.2265520095825195\npos emb time: 0.0006480216979980469\ndecoder time: 0.10525870323181152\nhead prepare input time: 0.0024759769439697266\nhead forward time: 0.6618561744689941\ntotal Fast3R forward time: 3.9971985816955566\nDataset: pt_piazzasanmarco_grandplace with 168 images.\nClustering images...\nFound 2 clusters.\nEstimating camera poses for each cluster...\nencode_images time: 2.942718982696533\npos emb time: 0.0006387233734130859\ndecoder time: 0.08505749702453613\nhead prepare input time: 0.0025517940521240234\nhead forward time: 0.6118199825286865\ntotal Fast3R forward time: 3.643256902694702\nencode_images time: 4.193765640258789\npos emb time: 0.0008602142333984375\ndecoder time: 0.14849042892456055\nhead prepare input time: 0.0032129287719726562\nhead forward time: 0.8931884765625\ntotal Fast3R forward time: 5.239874362945557\nDataset: pt_sacrecoeur_trevi_tajmahal with 225 images.\nClustering images...\nFound 3 clusters.\nEstimating camera poses for each cluster...\nencode_images time: 3.3205649852752686\npos emb time: 0.0007517337799072266\ndecoder time: 0.10131168365478516\nhead prepare input time: 0.0029685497283935547\nhead forward time: 0.6901998519897461\ntotal Fast3R forward time: 4.116499900817871\nencode_images time: 3.170070171356201\npos emb time: 0.0007805824279785156\ndecoder time: 0.09828305244445801\nhead prepare input time: 0.0023932456970214844\nhead forward time: 0.6598148345947266\ntotal Fast3R forward time: 3.9316861629486084\nencode_images time: 3.313380479812622\npos emb time: 0.0007386207580566406\ndecoder time: 0.1061248779296875\nhead prepare input time: 0.002715587615966797\nhead forward time: 0.7048952579498291\ntotal Fast3R forward time: 4.12831449508667\nDataset: pt_stpeters_stpauls with 200 images.\nClustering images...\nFound 2 clusters.\nEstimating camera poses for each cluster...\nencode_images time: 4.5381176471710205\npos emb time: 0.0006768703460693359\ndecoder time: 0.13008546829223633\nhead prepare input time: 0.003281116485595703\nhead forward time: 0.9243462085723877\ntotal Fast3R forward time: 5.596907138824463\nencode_images time: 4.159245252609253\npos emb time: 0.0007076263427734375\ndecoder time: 0.14147377014160156\nhead prepare input time: 0.0031960010528564453\nhead forward time: 0.8801429271697998\ntotal Fast3R forward time: 5.185089111328125\nDataset: amy_gardens with 200 images.\nClustering images...\nFound 7 clusters.\nEstimating camera poses for each cluster...\nencode_images time: 1.3992040157318115\npos emb time: 0.0007050037384033203\ndecoder time: 0.041383981704711914\nhead prepare input time: 0.0016379356384277344\nhead forward time: 0.3028123378753662\ntotal Fast3R forward time: 1.7461469173431396\nencode_images time: 0.6127405166625977\npos emb time: 0.0004963874816894531\ndecoder time: 0.03391528129577637\nhead prepare input time: 0.0007398128509521484\nhead forward time: 0.126861572265625\ntotal Fast3R forward time: 0.7750601768493652\nencode_images time: 0.07766151428222656\npos emb time: 0.0006737709045410156\ndecoder time: 0.07198667526245117\nhead prepare input time: 0.0005197525024414062\nhead forward time: 0.05728459358215332\ntotal Fast3R forward time: 0.20852327346801758\nencode_images time: 3.393784999847412\npos emb time: 0.0007691383361816406\ndecoder time: 0.0785057544708252\nhead prepare input time: 0.002593994140625\nhead forward time: 0.7051115036010742\ntotal Fast3R forward time: 4.181192636489868\nencode_images time: 0.06351065635681152\npos emb time: 0.0005853176116943359\ndecoder time: 0.03488302230834961\nhead prepare input time: 0.0006046295166015625\nhead forward time: 0.06293272972106934\ntotal Fast3R forward time: 0.162825345993042\nencode_images time: 0.06207895278930664\npos emb time: 0.0006320476531982422\ndecoder time: 0.0331423282623291\nhead prepare input time: 0.0006697177886962891\nhead forward time: 0.015279054641723633\ntotal Fast3R forward time: 0.11217832565307617\nDataset: fbk_vineyard with 163 images.\nClustering images...\nFound 12 clusters.\nEstimating camera poses for each cluster...\nencode_images time: 0.06303715705871582\npos emb time: 0.0005447864532470703\ndecoder time: 0.03612923622131348\nhead prepare input time: 0.0005309581756591797\nhead forward time: 0.025368928909301758\ntotal Fast3R forward time: 0.1258831024169922\nencode_images time: 0.07593512535095215\npos emb time: 0.0004775524139404297\ndecoder time: 0.033324241638183594\nhead prepare input time: 0.0005161762237548828\nhead forward time: 0.04448056221008301\ntotal Fast3R forward time: 0.15520215034484863\nencode_images time: 0.0590057373046875\npos emb time: 0.00045609474182128906\ndecoder time: 0.032257795333862305\nhead prepare input time: 0.0005195140838623047\nhead forward time: 0.04405951499938965\ntotal Fast3R forward time: 0.13667821884155273\nencode_images time: 0.06099295616149902\npos emb time: 0.0006244182586669922\ndecoder time: 0.033472299575805664\nhead prepare input time: 0.0005280971527099609\nhead forward time: 0.04444241523742676\ntotal Fast3R forward time: 0.14035606384277344\nencode_images time: 0.06566905975341797\npos emb time: 0.0006082057952880859\ndecoder time: 0.04397106170654297\nhead prepare input time: 0.0005502700805664062\nhead forward time: 0.029746532440185547\ntotal Fast3R forward time: 0.14083647727966309\nencode_images time: 0.05811905860900879\npos emb time: 0.00045108795166015625\ndecoder time: 0.030866384506225586\nhead prepare input time: 0.0005068778991699219\nhead forward time: 0.04297494888305664\ntotal Fast3R forward time: 0.13318610191345215\nencode_images time: 0.05723381042480469\npos emb time: 0.0004420280456542969\ndecoder time: 0.03197288513183594\nhead prepare input time: 0.0004899501800537109\nhead forward time: 0.01324319839477539\ntotal Fast3R forward time: 0.10364985466003418\nencode_images time: 0.05448150634765625\npos emb time: 0.0005619525909423828\ndecoder time: 0.031636953353881836\nhead prepare input time: 0.0005238056182861328\nhead forward time: 0.013831615447998047\ntotal Fast3R forward time: 0.10135817527770996\nencode_images time: 0.07069754600524902\npos emb time: 0.0007061958312988281\ndecoder time: 0.037725210189819336\nhead prepare input time: 0.0008795261383056641\nhead forward time: 0.016683578491210938\ntotal Fast3R forward time: 0.12726807594299316\nencode_images time: 0.05598306655883789\npos emb time: 0.0005345344543457031\ndecoder time: 0.03221774101257324\nhead prepare input time: 0.0005838871002197266\nhead forward time: 0.014073610305786133\ntotal Fast3R forward time: 0.1037297248840332\nencode_images time: 0.05963134765625\npos emb time: 0.0005409717559814453\ndecoder time: 0.03227591514587402\nhead prepare input time: 0.0005390644073486328\nhead forward time: 0.04445314407348633\ntotal Fast3R forward time: 0.137725830078125\nDataset: ETs with 22 images.\nClustering images...\nFound 2 clusters.\nEstimating camera poses for each cluster...\nencode_images time: 0.05829882621765137\npos emb time: 0.00046944618225097656\ndecoder time: 0.03270149230957031\nhead prepare input time: 0.0006093978881835938\nhead forward time: 0.016396760940551758\ntotal Fast3R forward time: 0.1087949275970459\nencode_images time: 0.4697601795196533\npos emb time: 0.00061798095703125\ndecoder time: 0.03307294845581055\nhead prepare input time: 0.0007607936859130859\nhead forward time: 0.10152769088745117\ntotal Fast3R forward time: 0.6060900688171387\nDataset: stairs with 51 images.\nClustering images...\nFound 5 clusters.\nEstimating camera poses for each cluster...\nencode_images time: 0.06127357482910156\npos emb time: 0.00041794776916503906\ndecoder time: 0.03281402587890625\nhead prepare input time: 0.0005805492401123047\nhead forward time: 0.047338247299194336\ntotal Fast3R forward time: 0.1427173614501953\nencode_images time: 0.05896615982055664\npos emb time: 0.00046944618225097656\ndecoder time: 0.03215289115905762\nhead prepare input time: 0.0005414485931396484\nhead forward time: 0.015876293182373047\ntotal Fast3R forward time: 0.1082906723022461\nencode_images time: 0.062039852142333984\npos emb time: 0.0004892349243164062\ndecoder time: 0.0337221622467041\nhead prepare input time: 0.0005860328674316406\nhead forward time: 0.04215812683105469\ntotal Fast3R forward time: 0.13928985595703125\nencode_images time: 0.06059002876281738\npos emb time: 0.0004894733428955078\ndecoder time: 0.03334927558898926\nhead prepare input time: 0.0005998611450195312\nhead forward time: 0.04463815689086914\ntotal Fast3R forward time: 0.13996458053588867\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Save data to CSV and Evaluate","metadata":{}},{"cell_type":"code","source":"def format_submission(df):\n    \"\"\"Formats the DataFrame into Kaggle's submission format.\"\"\"\n    return df[[\"dataset\", \"scene\", \"image\", \"rotation_matrix\", \"translation_vector\"]]\n\n# Save to CSV\nsubmission = format_submission(df_submission)\nsubmission.to_csv(\"train_results.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:49:43.884944Z","iopub.execute_input":"2025-04-16T17:49:43.885277Z","iopub.status.idle":"2025-04-16T17:49:43.901184Z","shell.execute_reply.started":"2025-04-16T17:49:43.885252Z","shell.execute_reply":"2025-04-16T17:49:43.900362Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from IPython.display import HTML\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='train_results.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:54:27.645934Z","iopub.execute_input":"2025-04-16T17:54:27.646255Z","iopub.status.idle":"2025-04-16T17:54:27.652247Z","shell.execute_reply.started":"2025-04-16T17:54:27.646232Z","shell.execute_reply":"2025-04-16T17:54:27.651515Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<a href=train_results.csv>Download CSV file</a>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom itertools import combinations\nfrom scipy.spatial.distance import cdist\n\ndef horn_method(src, dst):\n    \"\"\"Horn's absolute orientation algorithm for similarity transformation\"\"\"\n    src_centroid = np.mean(src, axis=0)\n    dst_centroid = np.mean(dst, axis=0)\n\n    src_centered = src - src_centroid\n    dst_centered = dst - dst_centroid\n\n    H = src_centered.T @ dst_centered\n    U, _, Vt = np.linalg.svd(H)\n    R = Vt.T @ U.T\n\n    if np.linalg.det(R) < 0:\n        Vt[-1,:] *= -1\n        R = Vt.T @ U.T\n\n    scale = np.linalg.norm(dst_centered) / np.linalg.norm(src_centered)\n    t = dst_centroid - scale * R @ src_centroid\n\n    return R, scale, t\n\ndef compute_mAA(pred_poses, gt_poses, thresholds):\n    \"\"\"Compute mAA for a single scene using RANSAC-like approach\"\"\"\n    pred_centers = np.array([-pose[0].T @ pose[1] for pose in pred_poses])\n    gt_centers = np.array([-pose[0].T @ pose[1] for pose in gt_poses])\n    N = len(pred_centers)\n    best_counts = np.zeros(len(thresholds))\n\n    # Try all possible triplets\n    for triplet in combinations(range(N), 3):\n        try:\n            R, scale, t = horn_method(pred_centers[list(triplet)], gt_centers[list(triplet)])\n            \n            # Apply transformation to all points\n            transformed = scale * (R @ pred_centers.T).T + t\n            # Count inliers for each threshold\n            distances = np.linalg.norm(transformed - gt_centers, axis=1, ord=1)\n            \n            # Refine transformation with all inliers\n            for i, thresh in enumerate(thresholds):\n                inliers = np.where(distances < thresh)[0]\n                if len(inliers) >= 3:\n                    R_refined, scale_refined, t_refined = horn_method(\n                        pred_centers[inliers], gt_centers[inliers])\n                    transformed_refined = scale_refined * (R_refined @ pred_centers.T).T + t_refined\n                    distances_refined = np.linalg.norm(transformed_refined - gt_centers, axis=1)\n                    new_inliers = np.where(distances_refined < thresh)[0]\n\n                    if len(new_inliers) > best_counts[i]:\n                        best_counts[i] = len(new_inliers)\n        except:\n            continue\n\n    # Calculate accuracy at each threshold\n    accuracies = best_counts / len(pred_centers)\n    return np.mean(accuracies)\n\ndef parse_pose(row, isPred, cluster):\n    \"\"\"Parse rotation matrix and translation vector from CSV row\"\"\"\n    if row['scene_pred'] != cluster:\n        return None\n    try:\n        R_col_name = 'rotation_matrix'\n        R_col_name += '_pred' if isPred else '_gt'\n        T_col_name = 'translation_vector'\n        T_col_name += '_pred' if isPred else '_gt'\n            \n        R = np.array(row[R_col_name].split(';'), dtype=float).reshape(3,3)\n        T = np.array(row[T_col_name].split(';'), dtype=float)\n        return (R, T)\n    except:\n        return None\n\ndef parse_threshold(row):\n    \"\"\"Parse threshold vector from CSV row\"\"\"\n    try:\n        thresholds = np.array(row['thresholds'].split(';'), dtype=float)\n        return thresholds\n    except:\n        return None\n\ndef evaluate_submission(pred_csv, gt_csv, thresholds_csv):\n    \"\"\"Full evaluation pipeline matching competition metrics\"\"\"\n    pred = pd.read_csv(pred_csv)\n    gt = pd.read_csv(gt_csv)\n    thresholds = pd.read_csv(thresholds_csv)\n\n    merged = pd.merge(pred, gt,\n                    on=['dataset', 'image'],\n                    suffixes=('_pred', '_gt'))\n\n    # Group by dataset and scene\n    gt_groups = merged.groupby(['dataset', 'scene_gt'])\n    pred_groups = merged.groupby(['dataset', 'scene_pred'])\n\n    results = []\n\n    for (dataset, scene), scene_data in gt_groups:\n        if scene == 'outliers':\n            continue\n\n        # Get ground truth poses\n        # gt_poses = [parse_pose(row, False) for _, row in scene_data.iterrows()]\n        # gt_poses = [p for p in gt_poses if p is not None]\n\n        gt_thresholds = parse_threshold(thresholds[(thresholds[\"dataset\"]==dataset) & (thresholds[\"scene\"]==scene)].iloc[0])\n        \n        # if len(gt_poses) < 3:\n        #     continue\n\n        # Find best matching predicted cluster\n        best_mAA = 0\n        best_cluster_size = 0\n        best_cluster_overlap = 0\n        best_cluster_score = 0\n        best_cluster = None\n\n        for (pred_ds, pred_cluster), pred_data in pred_groups:\n            if pred_ds != dataset or pred_cluster == 'outliers':\n                continue\n\n            # Get ground truth poses\n            gt_poses = [parse_pose(row, False, pred_cluster) for _, row in scene_data.iterrows()]\n            gt_poses = [p for p in gt_poses if p is not None]\n\n            # Get predicted poses\n            pred_poses = [parse_pose(row, True, pred_cluster) for _, row in scene_data.iterrows()]\n            pred_poses = [p for p in pred_poses if p is not None]\n\n            if len(gt_poses) != len(pred_poses):\n                print(f\"Error: Expected the number of ground truth cameras ({len(gt_poses)}) to match the number of user selected camers ({len(pred_poses)})\")\n                continue\n\n            if len(pred_poses) < 3:\n                continue\n\n            # Calculate intersection\n            common_images = set(scene_data['image']) & set(pred_data['image'])\n            cluster_score = len(common_images) / len(pred_data)\n\n            # Calculate mAA\n            try:\n                mAA = compute_mAA(pred_poses, gt_poses, gt_thresholds)\n            except:\n                mAA = 0\n                \n            if mAA > best_mAA or (mAA == best_mAA and cluster_score > best_cluster_score):\n                best_mAA = mAA\n                best_cluster_score = cluster_score\n                best_cluster = pred_cluster\n                best_cluster_overlap = len(common_images)\n                best_cluster_size = len(pred_data)\n\n        if best_cluster:\n            results.append({\n                'dataset': dataset,\n                'scene': scene,\n                'mAA': best_mAA,\n                'cluster_overlap': best_cluster_overlap,\n                'cluster_size': best_cluster_size,\n                'mAA_counts': best_mAA*best_cluster_size\n            })\n\n    # Aggregate results\n    if not results:\n        return {'mean_mAA': 0, 'mean_clustering_score': 0, 'final_score': 0}\n\n    df_results = pd.DataFrame(results)\n    df_results_by_dataset = df_results.groupby(\"dataset\")\n    agg_cluster = df_results_by_dataset[[\"mAA_counts\", \"cluster_overlap\", \"cluster_size\"]].sum()\n    agg_cluster_scores = (agg_cluster[\"cluster_overlap\"] / agg_cluster[\"cluster_size\"])\n    agg_mAA = (agg_cluster[\"mAA_counts\"] / agg_cluster[\"cluster_size\"])\n\n    output = pd.concat([agg_mAA, agg_cluster_scores], names=[\"mAA\", \"cluster_scores\"], axis=1)\n    output = output.set_axis([\"mAA\", \"cluster_scores\"], axis=1)\n    output[\"dataset_scores\"] = 2*output[\"mAA\"]*output[\"cluster_scores\"] / (output[\"mAA\"] + output[\"cluster_scores\"])\n\n    return output\n\n\nthresholds_file = f'{data_dir}/train_thresholds.csv'\nlabels_file = f'{data_dir}/train_labels.csv'\nresults = evaluate_submission('train_results.csv', labels_file, thresholds_file)\nprint(results)\nprint(f\"Final Competition Score: {np.mean(results['dataset_scores'].to_list())*100:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:49:46.588685Z","iopub.execute_input":"2025-04-16T17:49:46.589033Z","iopub.status.idle":"2025-04-16T17:52:45.777923Z","shell.execute_reply.started":"2025-04-16T17:49:46.589004Z","shell.execute_reply":"2025-04-16T17:52:45.777186Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n<ipython-input-20-07fbd6b5fb62>:22: RuntimeWarning: divide by zero encountered in scalar divide\n  scale = np.linalg.norm(dst_centered) / np.linalg.norm(src_centered)\n<ipython-input-20-07fbd6b5fb62>:23: RuntimeWarning: invalid value encountered in multiply\n  t = dst_centroid - scale * R @ src_centroid\n<ipython-input-20-07fbd6b5fb62>:23: RuntimeWarning: invalid value encountered in matmul\n  t = dst_centroid - scale * R @ src_centroid\n<ipython-input-20-07fbd6b5fb62>:40: RuntimeWarning: invalid value encountered in multiply\n  transformed = scale * (R @ pred_centers.T).T + t\n<ipython-input-20-07fbd6b5fb62>:46: RuntimeWarning: invalid value encountered in less\n  inliers = np.where(distances < thresh)[0]\n","output_type":"stream"},{"name":"stdout","text":"                                        mAA  cluster_scores  dataset_scores\ndataset                                                                    \nETs                                0.000000        0.863636        0.000000\namy_gardens                        0.000000        1.000000        0.000000\nfbk_vineyard                       0.032895        0.631579        0.062533\nimc2023_haiper                     0.000000        0.981481        0.000000\nimc2023_heritage                   0.047107        0.887324        0.089465\nimc2023_theather_imc2024_church    0.072368        1.000000        0.134969\nimc2024_dioscuri_baalshamin        0.016949        1.000000        0.033333\nimc2024_lizard_pond                0.015152        0.909091        0.029806\npt_brandenburg_british_buckingham  0.043704        1.000000        0.083747\npt_piazzasanmarco_grandplace       0.022146        0.994048        0.043327\npt_sacrecoeur_trevi_tajmahal       0.000000        1.000000        0.000000\npt_stpeters_stpauls                0.000000        0.995000        0.000000\nstairs                             0.111111        0.933333        0.198582\nFinal Competition Score: 5.20\n","output_type":"stream"}],"execution_count":20}]}